{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITMO DE OPTIMIZACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de librerías y definición de funciones\n",
    "\n",
    "Lo primero que haremos, es importar todas las librerías que se utilizaran a lo largo del Notebook y definir todas las funciones que usaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las librerías que se utilizarán en el Notebook\n",
    "\n",
    "# Librerías para manejo de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías para visualización gráfica\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Librerías de SciKit Learn para realizar Machine Learning\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Librerías para Boosting con datos desbalanceados\n",
    "try: \n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    print(\"Para correr este Notebook precisas instalar la librería XGBoost. Para esto, ingresa el siguiente comando en la consola de comandos: pip install xgboost\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Para correr este Notebook precisas instalar la librería XGBoost. Para esto, ingresa el siguiente comando en la consola de comandos: pip install xgboost\")\n",
    "    \n",
    "# Librería para manejo del tiempo\n",
    "import time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Función mostrar_resultados()\n",
    "\n",
    "Se utiliza para visualizar los resultados de una clasificación,\n",
    "\n",
    "** Argumentos\n",
    "    y_test: las etiquetas de prueba\n",
    "    y_pred: las etiquetas predichas mediante el modelo de clasificación\n",
    "\n",
    "** Devuelve\n",
    "    Impresión de la matriz de confusión y del reporte de clasificación\n",
    "\"\"\"\n",
    "def mostrar_resultados(y_test, y_pred):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), xticklabels=['NO', 'SI'], yticklabels=['NO', 'SI'], annot=True, fmt=\"d\")\n",
    "    plt.title(\"Matriz de Confusión\")\n",
    "    plt.ylabel('Clases reales')\n",
    "    plt.xlabel('Clases predichas')\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Función mostrar_analisis_PCA()\n",
    "\n",
    "Se utiliza para descomponer una matriz en sus componentes principales\n",
    "\n",
    "** Argumentos\n",
    "    X: matriz a analizar/descomponer\n",
    "\n",
    "** Devuelve\n",
    "    Componentes principales de X\n",
    "\"\"\"\n",
    "def analisis_PCA(X):\n",
    "    model = PCA(n_components = 3)\n",
    "    model.fit(X)\n",
    "    return model.transform(X)\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Función buscar_mejores_parametros()\n",
    "\n",
    "Se utiliza para buscar, aleatoriamente mediante RandomizedSearchCV(),\n",
    "la configuración de parámetros del clasificador que maximizan \n",
    "la precisión balanceada (balanced_accuracy).\n",
    "\n",
    "Esto se logra mediante la iteración en un rango específico de parámetros,\n",
    "que se definió aleatoriamente para sacarle mayor provecho a RandomizedSearchCV().\n",
    "\n",
    "** Argumentos\n",
    "    X_train: la matriz con los datos de entrenamiento\n",
    "    y_train: las etiquetas de los datos de entrenamiento\n",
    "    metodo: el método que sobre el que se iterarán los parámetros\n",
    "            RFC = RandomForestClassifier() -> método por default\n",
    "            DTC = DecisionTreeClassifier()\n",
    "            LR  = LogisticRegression()\n",
    "            GBC = GradientBoostingClassifier()\n",
    "            ETC = ExtraTreesClassifier()\n",
    "            ABC = AdaBoostClassifier()\n",
    "            XGB = XGBClassifier()\n",
    "    iters: número de iteraciones a realizar. Por default 4.\n",
    "\n",
    "** Devuelve\n",
    "    1. El estimador ya configurado con los parámetros que maximizan la métrica balanced_accuracy.\n",
    "    2. Los mejores parámetros encontrados.\n",
    "    3. El resultado de balanced_accuracy (maximizado).\n",
    "    4. Resultados de la validación cruzada\n",
    "\"\"\"\n",
    "def buscar_mejores_parametros(X_train, y_train, metodo = 'RFC', iters = 4):\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train.unique())\n",
    "    y_train_aux = le.transform(y_train)\n",
    "    \n",
    "    if metodo == 'RFC':\n",
    "        estimador = RandomForestClassifier(n_jobs = -1, random_state = 7953)\n",
    "    elif metodo == 'DTC':\n",
    "        estimador = DecisionTreeClassifier(random_state = 7953)\n",
    "    elif metodo == 'LR':\n",
    "        estimador = LogisticRegression(n_jobs = -1, random_state = 7953)\n",
    "    elif metodo == 'GBC':\n",
    "        estimador = GradientBoostingClassifier(random_state = 7953)\n",
    "    elif metodo == 'ETC':\n",
    "        estimador = ExtraTreesClassifier(n_jobs = -1, random_state = 7953)\n",
    "    elif metodo == 'ABC':\n",
    "        estimador = AdaBoostClassifier(random_state = 7953)\n",
    "    elif metodo == 'XGB':\n",
    "        relacion  = max(y_train.value_counts()) / min(y_train.value_counts())\n",
    "        estimador = XGBClassifier(n_jobs = -1, random_state = 7953, scale_pos_weight = relacion)\n",
    "\n",
    "    params = {\n",
    "                 'RFC' : {\n",
    "                             'n_estimators'     : np.random.randint(0, 100, size = 20),\n",
    "                             'criterion'        : ('gini', 'entropy'),\n",
    "                             'max_depth'        : np.random.randint(0,  75, size = 20),\n",
    "                             'min_samples_split': np.random.randint(0, 250, size = 20),\n",
    "                             'min_samples_leaf' : np.random.randint(0, 250, size = 20),\n",
    "                             'class_weight'     : ('balanced', 'balanced_subsample', None),\n",
    "                             'max_features'     : ('auto', 'sqrt', 'log2', None)\n",
    "                         },\n",
    "        \n",
    "                 'DTC' : {\n",
    "                             'criterion'        : ('gini', 'entropy'),\n",
    "                             'splitter'         : ('best', 'random'),\n",
    "                             'max_depth'        : np.random.randint(0, 100, size = 25),\n",
    "                             'min_samples_split': np.random.randint(0, 300, size = 25),\n",
    "                             'min_samples_leaf' : np.random.randint(0, 300, size = 25),\n",
    "                             'class_weight'     : ('balanced', None),\n",
    "                             'max_features'     : ('auto', 'sqrt', 'log2', None)\n",
    "                         },\n",
    "        \n",
    "                 'LR' : {\n",
    "                             'C'                : np.random.uniform(0, 1, size = 15),\n",
    "                             'fit_intercept'    : (True, False),\n",
    "                             'class_weight'     : ('balanced', None),\n",
    "                             'solver'           : ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "                             'max_iter'         : np.random.randint(0, 1000, size = 25),\n",
    "                             'l1_ratio'         : np.random.uniform(0, 1   , size = 15)\n",
    "                        },\n",
    "                \n",
    "                'GBC' : {\n",
    "                             'loss'             : ('deviance', 'exponential'),\n",
    "                             'learning_rate'    : np.random.uniform(0, 1  , size = 15),\n",
    "                             'n_estimators'     : np.random.randint(0, 150, size = 15),\n",
    "                             'subsample'        : np.random.uniform(0, 1  , size =  5),\n",
    "                             'criterion'        : ('friedman_mse', 'mse', 'mae'),\n",
    "                             'min_samples_split': np.random.randint(0, 150, size = 15),\n",
    "                             'min_samples_leaf' : np.random.randint(0, 150, size = 15),\n",
    "                             'max_features'     : ('auto', 'sqrt', 'log2', None),\n",
    "                             'warm_start'       : (True, False)\n",
    "                        },\n",
    "                \n",
    "                'ETC' : {\n",
    "                             'n_estimators'     : np.random.randint(0, 300, size = 25),\n",
    "                             'criterion'        : ('gini', 'entropy'),\n",
    "                             'max_depth'        : np.random.randint(0, 100, size = 25),\n",
    "                             'min_samples_split': np.random.randint(0, 300, size = 25),\n",
    "                             'min_samples_leaf' : np.random.randint(0, 300, size = 25),\n",
    "                             'bootstrap'        : (True, False),\n",
    "                             'class_weight'     : ('balanced', 'balanced_subsample', None)\n",
    "                        }, \n",
    "        \n",
    "                'ABC' : {\n",
    "                             'n_estimators'     : np.random.randint(1, 50, size = 15),\n",
    "                             'learning_rate'    : np.random.uniform(0, 1 , size = 15) \n",
    "                        },\n",
    "            \n",
    "                'XGB' : {\n",
    "                             'n_estimators'     : np.random.randint(1, 50, size = 20),\n",
    "                             'max_depth'        : np.random.randint(1, 40, size = 15),\n",
    "                             'learning_rate'    : np.random.uniform(0, 2 , size = 20),\n",
    "                             'objective'        : ('binary:logistic', 'binary:logitraw', 'reg:logistic'),\n",
    "                             'reg_lambda'       : np.random.uniform(0, 3 , size = 5),\n",
    "                             'reg_alpha'        : np.random.uniform(0, 3 , size = 5),\n",
    "                             'gamma'            : np.random.uniform(0, 10, size = 5),  \n",
    "                             'min_child_weight' : np.random.randint(1, 10, size = 10), \n",
    "                             'max_delta_step'   : np.random.randint(1, 10, size = 10)\n",
    "                        }\n",
    "             }\n",
    "\n",
    "    scorers = {\n",
    "                  'accuracy'         : 'accuracy',\n",
    "                  'balanced_accuracy': 'balanced_accuracy',\n",
    "                  'f1'               : 'f1',\n",
    "                  'neg_log_loss'     : 'neg_log_loss',\n",
    "                  'precision'        : 'precision',\n",
    "                  'recall'           : 'recall'\n",
    "              }\n",
    "\n",
    "    rs = RandomizedSearchCV(\n",
    "                                estimator           = estimador,\n",
    "                                param_distributions = params[metodo], \n",
    "                                scoring             = scorers, \n",
    "                                refit               = 'balanced_accuracy', \n",
    "                                n_iter              = iters, \n",
    "                                n_jobs              = -1,\n",
    "                                random_state        = 7953,\n",
    "                                cv                  = 10\n",
    "                           )\n",
    "\n",
    "    rs.fit(X_train, y_train_aux)\n",
    "    \n",
    "    return rs.best_estimator_, rs.best_params_, rs.best_score_, rs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga del Dataset y exploración de datos\n",
    "Ahora, cargamos el dataset dentro de un DataFrame de Pandas, y visualizamos algunos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el DataFrame a partir del DataSet enviado\n",
    "mkt = pd.read_csv('mkt_bank.csv')\n",
    "\n",
    "# Inspeccionamos la estructura de los datos\n",
    "mkt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miramos qué clase de datos contiene cada columna y si están completas o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos fijamos si todas las columnas están completas\n",
    "# y qué tipos de datos contienen\n",
    "mkt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos un análisis estadístico\n",
    "mkt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tener una idea de la cantidad de datos categóricos desconocidos que hay en el DataFrame, imprimimos, columna por columna, cuántas veces aparece el string \"desconocido\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero filtramos todas las columnas que tienen valores categóricos\n",
    "cols_categ = mkt.dtypes == object\n",
    "\n",
    "# Luego, obtenemos los nombres de estas columnas\n",
    "cols_categ_index = cols_categ.loc[cols_categ.values == True].index\n",
    "\n",
    "for categ in cols_categ_index:\n",
    "    print('Cantidad de desconocidos en la columna \"' + str(categ) + '\": ' + str(mkt[mkt[categ] == 'desconocido'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Verificación de la distribución de clases\n",
    "\n",
    "Lo siguiente se hace para tener una idea de la distribución de clases que tiene el dataset. Como se aprecia, sólo el 11% son positivos, por lo que esto nos da una idea del modelo nulo y del alto desbalanceo de la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidades por clase en el dataset: ')\n",
    "print(mkt['y'].value_counts())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Proporción de clases en el dataset: ')\n",
    "print(mkt['y'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-procesamiento de datos\n",
    "\n",
    "Realizamos a continuación un pre-procesamiento de los datos, con el fin de mejorar la performance del método de clasificación que usaremos luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la primer columna del DataFrame, que no es útil para el análisis\n",
    "mkt = mkt.drop(columns = ['Unnamed: 0'], axis = 1)\n",
    "\n",
    "# Asignamos la variable objetivo\n",
    "y = mkt['y']\n",
    "\n",
    "# Luego la eliminamos del DataFrame\n",
    "mkt = mkt.drop(columns = ['y'], axis = 1)\n",
    "\n",
    "# Filtramos todas las columnas que tienen valores categóricos\n",
    "cols_categ = mkt.dtypes == object\n",
    "\n",
    "# Luego, obtenemos los nombres de estas columnas\n",
    "cols_categ_index = cols_categ.loc[cols_categ.values == True].index\n",
    "\n",
    "# Vamos a utilizar LabelEncoder() para pasar a numéricas todas las variables categóricas\n",
    "\n",
    "# En el siguiente bucle, transformamos todas las variables con LabelEncoder()\n",
    "for col in cols_categ_index:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(mkt[col].unique())\n",
    "    mkt[col] = le.transform(mkt[col])\n",
    "\n",
    "# En la columna 'Dias', 999 significa que el cliente no fue contactado previamente.\n",
    "# Resulta razonable entonces convertirlo a cero para uniformizar la información\n",
    "mkt.replace({'Dias': {999: 0}}, inplace = True)\n",
    "\n",
    "# Vemos cómo quedó el DataFrame\n",
    "mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos a 'X' las variables explicativas\n",
    "X = mkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Estandarización\n",
    "\n",
    "Hacemos que todas las variables tengan media nula y desvío unitario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizamos las variables cuantitativas con StandardScaler() (media = 0 y desvío = 1)\n",
    "cl = ColumnTransformer(transformers = [('Scaler', StandardScaler(), X.columns)],\n",
    "                       remainder='passthrough')\n",
    "\n",
    "# Ejecutamos la transformación y visualizamos cómo quedó\n",
    "X = cl.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Separación de los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "Realizamos la separación mediante el siguiente criterio: 80% de los datos para entrenamiento y 20% para prueba.\n",
    "Dado el alto desbalance de clases presente, utilizamos estratificación en el método, para obtener conjuntos con las mismas proporciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos en sets de entrenamiento y prueba, usando\n",
    "# estratificación, dada la gran disparidad de clases.\n",
    "# Separamos el 20% de los datos para entrenamiento.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                       X, y,\n",
    "                                                       test_size    = 0.2,\n",
    "                                                       stratify     = y,\n",
    "                                                       random_state = 7953\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Búsqueda de parámetros óptimos\n",
    "\n",
    "Usaremos la función buscar_mejores_parametros() para optimizar los clasificadores.\n",
    "\n",
    "Para esto, debemos seleccionar el clasificador que usaremos y la cantidad de iteraciones a realizar.\n",
    "\n",
    "##### ADVERTENCIA: Un gran número de iteraciones puede llegar a demorar mucho en finalizar, en función de la potencia de cálculo disponible. Para probar, ver cuánto demora con 1 iteración para tener una idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos la clase que usaremos para optimizar sus parámetros.\n",
    "# XGB = XGBClassifier()\n",
    "clase = 'XGB'\n",
    "\n",
    "# Elegimos la cantidad de iteraciones a realizar en la optimización\n",
    "iteraciones = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos el tiempo inicial\n",
    "tic = time.time()\n",
    "\n",
    "# Buscamos los mejores parámetros para Boosting\n",
    "xgb, mejores_params, mejor_scorer, cv_res = buscar_mejores_parametros(\n",
    "                                                                         X_train,\n",
    "                                                                         y_train,\n",
    "                                                                         metodo = clase,\n",
    "                                                                         iters  = iteraciones\n",
    "                                                                     )\n",
    "# Tomamos el tiempo final\n",
    "toc = time.time()\n",
    "\n",
    "print('Demora de buscar_mejores_parametros() para XGB: {seg:.2f} segundos ({mins:.2f} minutos)\\n'.format(seg = toc-tic, mins = (toc-tic)/60))\n",
    "\n",
    "# Visualizamos cuáles son los mejores parámetros y el scorer\n",
    "print('Los mejores parámetros de XGB son: ')\n",
    "print(mejores_params)\n",
    "print('\\nEl máximo scorer de XGB dió: ' + str(mejor_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos los resultados de la validación cruzada\n",
    "print(\"Resultados de la validación cruzada con estratificación\")\n",
    "pd.DataFrame(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos al modelo optimizado, predecimos y visualizamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos al modelo\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones y visualizamos los resultados\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print('Los resultados con XGBClassifier() son: \\n')\n",
    "\n",
    "# Vemos la exactitud balanceada, útil para los casos de alto desbalanceo de clases\n",
    "print('El accuracy balanceado de entrenamiento es de: {}'.format(balanced_accuracy_score(y_train, xgb.predict(X_train))))\n",
    "print('El accuracy balanceado de prueba es de: {}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# Vemos la exactitud convencional\n",
    "print('El accuracy de entrenamiento es de: {}'.format(accuracy_score(y_train, xgb.predict(X_train))))\n",
    "print('El accuracy de prueba es de: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "mostrar_resultados(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos las probabilidades\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "p_tp = tp / (tp + fn) # Probabilidad de positivo real\n",
    "p_fn = fn / (tp + fn) # Probabilidad de falso negativo\n",
    "p_tn = tn / (tn + fp) # Probabilidad de negativo real\n",
    "p_fp = fp / (tn + fp) # Probabilidad de falso positivo\n",
    "\n",
    "# Luego las imprimimos\n",
    "print('Probabilidad de positivo real: {proba:.2f} %'.format(proba = p_tp*100))\n",
    "print('Probabilidad de falso negativo: {proba:.2f} %'.format(proba = p_fn*100))\n",
    "print('Probabilidad de negativo real: {proba:.2f} %'.format(proba = p_tn*100))\n",
    "print('Probabilidad de falso positivo: {proba:.2f} %'.format(proba = p_fp*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos el costo/beneficio de cada posibilidad\n",
    "b_tp = 2900\n",
    "b_tn = 200\n",
    "c_fp = -440\n",
    "c_fn = 0\n",
    "\n",
    "# Visualizamos la probabilidad de cada caso\n",
    "proporcion = y.value_counts(normalize = True)\n",
    "\n",
    "# Finalmente, calculamos la esperanza\n",
    "clase_0 = proporcion[0]\n",
    "clase_1 = proporcion[1]\n",
    "valor_esperado = clase_1 * (p_tp * b_tp + p_fn * c_fn) + clase_0 * (p_tn * b_tn + p_fp * c_fp)\n",
    "valor_esperado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
